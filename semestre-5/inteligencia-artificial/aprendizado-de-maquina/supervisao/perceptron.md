# Perceptron

As <mark style="color:purple;">redes neurais</mark> usam um modelo computacional de neurÃ´nio chamado&#x20;Perceptron. Ele Ã© uma estrutura inspirada em um neurÃ´nio (entrada de sinal e ativaÃ§Ã£o), que resulta em **zero ou um**, e pode ser representada por uma fÃ³rmula:

$$
ğ‘¦ = ğ‘“ (âˆ‘ ğ‘Šğ‘– âˆ— ğ‘‹ğ‘– + ğ‘Šğ‘ âˆ— 1)
$$

<details>

<summary>ExplicaÃ§Ã£o da fÃ³rmula</summary>

* f() uma funÃ§Ã£o degrau, forÃ§ando o resultado a ser 0 ou 1.
* ğ‘Šğ‘  &#x20;Ã© um peso para uma entrada especial que serÃ¡ discutida em breve.
* A parte interna Ã© uma soma ponderada das entradas, vamos entender como isso  &#x20;funciona, pensando em uma Ãºnica entrada.

</details>

Essa fÃ³rmula gera uma reta no plano cartesiano:

* Ao alterar o peso Wb, a angulatura desta reta se altera.
* Ao alterar o viÃ©s[^1], mudamos por onde a reta passa. Caso contrÃ¡rio, ela passa pela origem (0,0).

Essa linha Ã© usada para separar os grupos em seus cortes. Alteraramos o peso ou o viÃ©s para ir ajustando as linhas e fazer cortes precisos na separaÃ§Ã£o dos grupos. Em resumo, <mark style="color:blue;">cada linha Ã© um neurÃ´nio</mark>.

Muitas vezes nÃ£o Ã© possÃ­vel dividir os grupos com apenas uma linha, entÃ£o podemos inserir mais linhas para criar novas divisÃµes. Curiosidade: nÃ£o pensar em usar duas linhas foi o que travou o desenvolvimento da IA a muitos anos atrÃ¡s.

<figure><img src="../../../../.gitbook/assets/image (1).png" alt="" width="263"><figcaption></figcaption></figure>

Os pesos sÃ£o iniciados com valores aleatÃ³rios e sendo ajustados gradualmente. Esse Ã© um exemplo final de perceptron fazendo uma divisÃ£o de grupos.

<figure><img src="../../../../.gitbook/assets/image (2).png" alt="" width="309"><figcaption></figcaption></figure>

[^1]: Sinal/dendrito do neurÃ´nio, que Ã© constantemente igual a 1 e estarÃ¡ associado a um    &#x20;peso ğ‘Šb.
